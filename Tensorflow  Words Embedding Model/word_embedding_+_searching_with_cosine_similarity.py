# -*- coding: utf-8 -*-
"""Word Embedding + Searching with Cosine Similarity.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GS5F685che5WcaQp_PkFFOYmnMa3QHYU
"""

import csv
import chardet
import re
import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.metrics.pairwise import cosine_similarity
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')

from google.colab import drive
drive.mount('/content/drive')

# Step 1: Load GloVe word embeddings
embeddings_index = {}
with open('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/glove.6B.100d.txt', 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Step 2: Create embedding matrix and vocabulary
embedding_dim = 100  # Dimensionality of the word embeddings
embedding_matrix = np.zeros((len(embeddings_index), embedding_dim))
vocab = []

for i, word in enumerate(embeddings_index):
    embedding_vector = embeddings_index[word]
    embedding_matrix[i] = embedding_vector
    vocab.append(word)

# Step 3: Convert embedding matrix to TensorFlow embedding
embedding_layer = tf.keras.layers.Embedding(
    len(embeddings_index),
    embedding_dim,
    weights=[embedding_matrix],
    trainable=False
)

# Step 4: Test the TensorFlow embedding
word_input = tf.keras.Input(shape=(1,), dtype=tf.int32)
embedding_output = embedding_layer(word_input)

# Step 5: Build and compile a model that uses the embedding layer
model = tf.keras.models.Sequential([
    embedding_layer,
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Example
word = "machine"
word_index = vocab.index(word)
embedded_word = embedding_matrix[word_index]
print(embedded_word)

# Step 6: Prepare the search data
search_data = []
for word in vocab:
    search_data.append(embeddings_index[word])

search_data = np.array(search_data)

# Step 7: Define the search function
def search(query, top_k=5):
    top_words_list = []
    for query_word in query:
        query_tokens = query_word.split()
        query_embedding = np.mean([embeddings_index[token] for token in query_tokens if token in embeddings_index], axis=0)
        similarity_scores = cosine_similarity([query_embedding], search_data)
        similarity_scores = similarity_scores.reshape(-1)
        top_indices = similarity_scores.argsort()[-top_k:][::-1]
        top_words = [vocab[i] for i in top_indices]
        top_words_list.append(top_words)
    return top_words_list

stop_words = set(stopwords.words('english'))

# Words to remove
additional_keywords = ["caffe", "place", "coffee", "nan", "cafe"]

# Asking User Input
user_input = input("Search: ")

# Split the input into individual words
words = user_input.split()

# Preprocess the search query by splitting and removing symbols
search_keywords = re.findall(r'\b\w+\b', user_input.lower())

# Remove stop words and additional keywords from the search keywords
search_keywords = [word for word in search_keywords if word not in stop_words and word not in additional_keywords]

# Join the remaining words back together

print("Filtered Input:", search_keywords)

top_words_list = search(search_keywords, top_k=10)

list_of_words = []
for i, query_word in enumerate(search_keywords):
    print(f"Closest meanings for query '{query_word}':")
    print(top_words_list[i])
    list_of_words.append(top_words_list[i])
    print()
list_of_words  = [word for sublist in list_of_words for word in sublist]
# list_of_words = list_of_words[0:len(query)]
print('List closest keyword from query: \n', list_of_words)

def search_keywords(csv_file, keywords, column):

  with open(csv_file, 'r') as f:
    reader = csv.reader(f)
    rows = []
    for row in reader:
      rows.append(row)

  row_numbers = []
  for row in rows:
    for keyword in keywords:
      if keyword in row[column]:
        row_numbers.append(rows.index(row))

  return row_numbers


if __name__ == '__main__':
    csv_file = '/content/drive/MyDrive/For Capstone/Collecting data/Place Detail (Scored + Keyword 1 & 2 Extracted  + Additional Feature (longlang, contact etc)).csv'
    keywords = list_of_words
    column = 13

    row_numbers = search_keywords(csv_file, keywords, column)
    unique_list = list(set(row_numbers))
    sorted_list = sorted(unique_list)
    Place_list= sorted_list[:20]

print(Place_list)

def caffe_result(Place_list):
  columns_to_extract = [0, 2, 4, 5, 14]
  output = []

  with open(csv_file, 'r') as file:
      reader = csv.reader(file)
      next(reader)  # Skip the header row

      def get_data(row_numbers, column_numbers):
          data = []
          for i, row in enumerate(reader):
              if i + 1 in row_numbers:
                  row_data = [row[col] for col in column_numbers]
                  data.append(row_data)
          return data

      data = get_data(Place_list, columns_to_extract)
      for row in data:
          output.append({
                "name": row[0],
                "address": row[1],
                "rating": float(row[2]),
                "total_review": int(row[3]),
                "url_photo": row[4]
            })

      return output

# print(caffe_result(Place_list))


results = caffe_result(Place_list)
for result in results:
    print(result)

print(type(result))

model.save('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/Word_Embedding.h5')

# Load your trained TensorFlow model
model = tf.keras.models.load_model('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/Word_Embedding.h5')  # Replace 'your_model.h5' with your model file

# Convert the model to JSON
model_json = model.to_json()

# Save the JSON model to a file
with open('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/Word_Embedding.json', 'w') as json_file:
    json_file.write(model_json)

# !cp -r "/content/Word_Embedding.h5" "/content/drive/MyDrive/For Capstone"
#!cp -r "/content/Word_Embedding.json" "/content/drive/MyDrive/For Capstone"

export_dir = '/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding'

# YOUR CODE HERE
tf.saved_model.save(model, export_dir = export_dir)

# Select mode of optimization
mode = "Speed" 

if mode == 'Storage':
    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE
elif mode == 'Speed':
    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY
else:
    optimization = tf.lite.Optimize.DEFAULT

converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)

# Set the optimzations
converter.optimizations = [optimization]

# Invoke the converter to finally generate the TFLite model
tflite_model = converter.convert()

import pathlib

tflite_model_file = pathlib.Path('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/model for Word Embedding + Searching with Cosine Similarity.tflite')
tflite_model_file.write_bytes(tflite_model)



#!cp -r "/content/model for Word Embedding + Searching with Cosine Similarity.tflite" "/content/drive/MyDrive/For Capstone"