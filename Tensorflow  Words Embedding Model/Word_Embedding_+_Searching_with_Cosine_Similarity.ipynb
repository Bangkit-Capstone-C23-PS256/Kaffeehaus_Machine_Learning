{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Documentation**\n",
        "\n",
        "## Dependencies\n",
        "The code relies on the following libraries and modules:\n",
        "\n",
        "- `csv`: Provides functionality for reading and writing CSV files.\n",
        "- `chardet`: Enables character encoding detection.\n",
        "- `re`: Offers regular expression operations.\n",
        "- `pandas`: A powerful library for data manipulation and analysis.\n",
        "- `numpy`: Provides support for mathematical operations on arrays and matrices.\n",
        "- `sklearn`: A comprehensive machine learning library.\n",
        "- `nltk`: A natural language processing library.\n",
        "\n",
        "##Functionality\n",
        "\n",
        "1. Reading CSV Files\n",
        "\n",
        "  The code utilizes the `csv` module to read CSV files. It allows for the extraction of data from these files for further analysis.\n",
        "\n",
        "2. Character Encoding Detection\n",
        "\n",
        "  The `chardet` library is used to detect the character encoding of the text data. It helps ensure that the data is properly decoded and processed.\n",
        "\n",
        "3. Regular Expression Operations\n",
        "\n",
        "  The `re` module enables the use of regular expressions. Regular expressions are powerful tools for pattern matching and text manipulation.\n",
        "\n",
        "4. Data Manipulation with pandas and numpy\n",
        "\n",
        "  The code utilizes the `pandas` library for efficient data manipulation and analysis. It allows for easy handling of structured data, such as CSV files. The numpy library is used for mathematical operations on arrays and matrices.\n",
        "\n",
        "5. Cosine Similarity Calculation\n",
        "\n",
        "  The code employs the `cosine_similarity` function from the `sklearn.metrics.pairwise` module to calculate the cosine similarity between two vectors. Cosine similarity is a measure of similarity between two non-zero vectors.\n",
        "\n",
        "6. Natural Language Processing (NLP) with nltk\n",
        "\n",
        "  The code uses the `nltk` library for natural language processing tasks. It specifically utilizes the stopwords corpus from the `nltk.corpus` module. Stopwords are common words that are often excluded from text analysis as they typically do not carry important meaning."
      ],
      "metadata": {
        "id": "4qtsdn97QIJq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rJHVbdeA3_hU"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import chardet\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw10ciJSMqdI",
        "outputId": "4e7178c1-74e7-4db3-c22c-e6bc074a8ef4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "libraries = {\n",
        "    'csv': csv,\n",
        "    'chardet': chardet,\n",
        "    're': re,\n",
        "    'pandas': pd,\n",
        "    'numpy': np,\n",
        "    'sklearn': sklearn,\n",
        "    'nltk': nltk,\n",
        "}\n",
        "\n",
        "for library_name, library in libraries.items():\n",
        "    try:\n",
        "        version = library.__version__\n",
        "        print(f'{library_name}: {version}')\n",
        "    except AttributeError:\n",
        "        print(f'{library_name}: version not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCNQ57va4IpF",
        "outputId": "29ba93f4-ace6-4bcc-899a-36ef0138643a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "csv: 1.0\n",
            "chardet: 4.0.0\n",
            "re: 2.2.1\n",
            "pandas: 1.5.3\n",
            "numpy: 1.22.4\n",
            "sklearn: 1.2.2\n",
            "nltk: 3.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports\n",
        "\n",
        "The following library is imported in the code:\n",
        "\n",
        "- `google.colab`: A library for working with Google Colab, a cloud-based Jupyter notebook environment.\n",
        "\n",
        "## Functionality\n",
        "\n",
        "1. The code snippet uses the `google.colab` library to mount the Google Drive to the Colab runtime. This allows accessing and working with files and directories in the Google Drive within the Colab environment.\n",
        "\n",
        "2. The `drive.mount()` function is called with the /content/drive directory as the mount point, where the Google Drive will be mounted. Once the Google Drive is successfully mounted, the files and directories within it can be accessed using standard file I/O operations.\n"
      ],
      "metadata": {
        "id": "uyGjU9AMRaLI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x44av-xVGBke",
        "outputId": "3d4c6d58-1b7f-41ce-e62e-e33d8a47172b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the Word Embedding File\n",
        "\n",
        "The code reads the word embedding file in the `GloVe` format. The file path is specified as `/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/glove.6B.100d.txt`. The file is opened in read mode with the specified encoding of `utf-8`.\n",
        "\n",
        "## Creating the Embeddings Index\n",
        "The code iterates over each line in the file. Each line represents a word and its corresponding embedding vector. The line is split into a list of values based on whitespace.\n",
        "\n",
        "- `word`: The first value in the line represents the word itself.\n",
        "- `coefs`: The remaining values in the line represent the embedding vector as a sequence of floating-point numbers.\n",
        "\n",
        "The code converts the embedding vector values to a `NumPy` array of type `float32` and assigns it to the variable coefs.\n",
        "\n",
        "The word and its corresponding embedding vector are added as a `key-value` pair to the `embeddings_index` dictionary."
      ],
      "metadata": {
        "id": "LzJ6QqI3RdPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load GloVe word embeddings\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n"
      ],
      "metadata": {
        "id": "DL8enUU24G6w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting the Embedding Dimension\n",
        "The code initializes the variable `embedding_dim` with a value of 100. This variable represents the dimensionality of the word embeddings.\n",
        "\n",
        "## Creating the Embedding Matrix\n",
        "- The code initializes an empty `NumPy` array called `embedding_matrix` with dimensions `(len(embeddings_index), embedding_dim)`. This matrix will store the word embeddings for the given vocabulary.\n",
        "\n",
        "- The code also initializes an empty list called `vocab` to store the words corresponding to each row of the embedding matrix.\n",
        "\n",
        "- The code then iterates over the words in the `embeddings_index` dictionary using the enumerate function. The enumerate function provides an index i and the corresponding word at each iteration.\n",
        "\n",
        "- For each word, the code retrieves the embedding vector from the `embeddings_index` dictionary using the word as the key.\n",
        "\n",
        "- The embedding vector is assigned to the `embedding_vector` variable.\n",
        "\n",
        "- The `embedding_vector` is then assigned to the ith row of the `embedding_matrix`, effectively storing the embedding vector in the matrix.\n",
        "\n",
        "- The `word` is appended to the vocab list."
      ],
      "metadata": {
        "id": "E3X8VYsGSFy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create embedding matrix and vocabulary\n",
        "embedding_dim = 100  # Dimensionality of the word embeddings\n",
        "embedding_matrix = np.zeros((len(embeddings_index), embedding_dim))\n",
        "vocab = []\n",
        "\n",
        "for i, word in enumerate(embeddings_index):\n",
        "    embedding_vector = embeddings_index[word]\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    vocab.append(word)"
      ],
      "metadata": {
        "id": "q0GOW9aIHCkz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the Search Data Array\n",
        "\n",
        "- The code initializes an empty list called `search_data` to store the embedding vectors for the `words` in the vocab list.\n",
        "\n",
        "- The code then iterates over each word in the `vocab` list.\n",
        "\n",
        "- For each word, the code retrieves the corresponding embedding vector from the `embeddings_index` dictionary using the word as the key.\n",
        "\n",
        "- The embedding vector is appended to the `search_data` list.\n",
        "\n",
        "- After iterating through all the words, the search_data list is converted to a `NumPy` array using `np.array(search_data)`."
      ],
      "metadata": {
        "id": "b05RPlTvSq3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare the search data\n",
        "search_data = []\n",
        "for word in vocab:\n",
        "    search_data.append(embeddings_index[word])\n",
        "\n",
        "search_data = np.array(search_data)\n"
      ],
      "metadata": {
        "id": "wNt0kQ2KLKSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the search function\n",
        "def search(query, top_k=5):\n",
        "    top_words_list = []\n",
        "    for query_word in query:\n",
        "        query_tokens = query_word.split()\n",
        "        query_embedding = np.mean([embeddings_index[token] for token in query_tokens if token in embeddings_index], axis=0)\n",
        "        similarity_scores = cosine_similarity([query_embedding], search_data)\n",
        "        similarity_scores = similarity_scores.reshape(-1)\n",
        "        top_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
        "        top_words = [vocab[i] for i in top_indices]\n",
        "        top_words_list.append(top_words)\n",
        "    return top_words_list"
      ],
      "metadata": {
        "id": "iYviKf_qLN2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop Words\n",
        "The code imports the stopwords corpus from the `nltk` library, specifically the English stop words. Stop words are common words that are often excluded from text analysis as they typically do not carry important meaning.\n",
        "\n",
        "## Additional Keywords\n",
        "The code defines a list called `additional_keywords` that contains additional words to be removed from the search query. These words are \"caffe\", \"place\", \"coffee\", \"nan\", and \"cafe\".\n",
        "\n",
        "## Preprocessing the Search Query\n",
        "The code splits the user input query into individual words using the `split` function.\n",
        "\n",
        "The code then preprocesses the search query by converting it to lowercase using the lower function and extracting only alphanumeric characters using the `re.findall` function. This removes symbols and special characters from the search query, keeping only words.\n",
        "\n",
        "## Removing Stop Words and Additional Keywords\n",
        "The code removes stop words and additional keywords from the search keywords list using list comprehension. It checks if each word is not in the `stop_words` set and not in the `additional_keywords` list.\n",
        "\n",
        "## Returning the Search Keywords\n",
        "The code `returns` the preprocessed search keywords as a list."
      ],
      "metadata": {
        "id": "X_xEzFlQX91X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_keyword(user_input):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # Words to remove\n",
        "    additional_keywords = [\"caffe\", \"place\", \"coffee\", \"nan\", \"cafe\"]\n",
        "\n",
        "    # Split the input into individual words\n",
        "    words = user_input.split()\n",
        "\n",
        "    # Preprocess the search query by splitting and removing symbols\n",
        "    search_keywords = re.findall(r'\\b\\w+\\b', user_input.lower())\n",
        "\n",
        "    # Remove stop words and additional keywords from the search keywords\n",
        "    search_keywords = [word for word in search_keywords if word not in stop_words and word not in additional_keywords]\n",
        "\n",
        "    return search_keywords\n",
        "\n",
        "# Asking User Input\n",
        "user_input = input(\"Search: \")\n",
        "result = input_keyword(user_input)\n",
        "print(\"Filtered Input:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6uRo-8jMUBP",
        "outputId": "97632474-8292-4a0e-9008-4353b5592621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search: caffe that has a affordable and cozy place\n",
            "Filtered Input: ['affordable', 'cozy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieving Top Words for Each Query Word\n",
        "The code calls a function named search with the result and `top_k` parameters to retrieve the top words for each query word. The specific implementation of the search function is not provided in the code snippet.\n",
        "\n",
        "The resulting top words for each query word are stored in the `top_words_list` variable.\n",
        "\n",
        "## Combining Top Words into a Single List\n",
        "The code initializes an empty list called `list_of_words` to store the combined top words.\n",
        "\n",
        "The code then iterates over the query words in the result list using the enumerate function. The enumerate function provides an index i and the corresponding `query_word` at each iteration.\n",
        "\n",
        "For each query word, the code prints a message indicating the closest meanings for that query word and prints the corresponding top words from the `top_words_list`. The top words for the query word are also appended to the `list_of_words`.\n",
        "\n",
        "## Flattening the List of Lists\n",
        "After iterating through all the query words, the code uses list comprehension to flatten the `list_of_words` by iterating over each sublist in `list_of_words` and extracting each word.\n",
        "\n",
        "The resulting flattened list of words is stored in the `list_of_words` variable."
      ],
      "metadata": {
        "id": "5USI0dvmbD76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_list = search(result, top_k=10)\n",
        "\n",
        "list_of_words = []\n",
        "for i, query_word in enumerate(result):\n",
        "    print(f\"Closest meanings for query '{query_word}':\")\n",
        "    print(top_words_list[i])\n",
        "    list_of_words.append(top_words_list[i])\n",
        "    print()\n",
        "list_of_words  = [word for sublist in list_of_words for word in sublist]\n",
        "# list_of_words = list_of_words[0:len(query)]\n",
        "print('List closest keyword from query: \\n', list_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWnDxY5MM--k",
        "outputId": "598c1e07-920b-4e30-fdd1-ffe9fe69bd25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest meanings for query 'affordable':\n",
            "['affordable', 'inexpensive', 'cheap', 'expensive', 'cheaper', 'efficient', 'unaffordable', 'accessible', 'priced', 'environmentally']\n",
            "\n",
            "Closest meanings for query 'cozy':\n",
            "['cozy', 'cosy', 'comfy', 'homey', 'cramped', 'dingy', 'spacious', 'clubby', 'rustic', 'shabby']\n",
            "\n",
            "List closest keyword from query: \n",
            " ['affordable', 'inexpensive', 'cheap', 'expensive', 'cheaper', 'efficient', 'unaffordable', 'accessible', 'priced', 'environmentally', 'cozy', 'cosy', 'comfy', 'homey', 'cramped', 'dingy', 'spacious', 'clubby', 'rustic', 'shabby']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading the CSV File\n",
        "The code opens the specified CSV file in read mode using the `open` function. It creates a CSV reader object using `csv.reader` to read the file contents.\n",
        "\n",
        "The code initializes an empty list called rows to store the rows of the CSV file.\n",
        "\n",
        "It then iterates over each row in the CSV file using a for loop. Each row is appended to the rows list.\n",
        "\n",
        "## Searching for Keywords\n",
        "The code initializes an empty list called `row_numbers` to store the row numbers where the keywords are found.\n",
        "\n",
        "The code iterates over each row in the rows list using a nested for loop. It also iterates over each keyword in the keywords list.\n",
        "\n",
        "For each row and keyword, the code checks if the keyword is present in the specified column of the row. If the keyword is found, the index of the row in the rows list is appended to the `row_numbers` list using the `index` method.\n",
        "\n",
        "## Returning the Row Numbers\n",
        "The code returns the `row_numbers` list, which contains the row numbers where the keywords were found."
      ],
      "metadata": {
        "id": "4Zn8J-Tobnnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_keywords(csv_file, keywords, column):\n",
        "\n",
        "  with open(csv_file, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = []\n",
        "    for row in reader:\n",
        "      rows.append(row)\n",
        "\n",
        "  row_numbers = []\n",
        "  for row in rows:\n",
        "    for keyword in keywords:\n",
        "      if keyword in row[column]:\n",
        "        row_numbers.append(rows.index(row))\n",
        "\n",
        "  return row_numbers\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    csv_file = '/content/drive/MyDrive/For Capstone/Collecting data/Place Detail (Scored + Keyword 1 & 2 Extracted  + Additional Feature (longlang, contact etc)).csv'\n",
        "    keywords = list_of_words\n",
        "    column = 13\n",
        "\n",
        "    row_numbers = search_keywords(csv_file, keywords, column)\n",
        "    unique_list = list(set(row_numbers))\n",
        "    sorted_list = sorted(unique_list)\n",
        "    Place_list= sorted_list[:20]\n",
        "\n",
        "print(Place_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPcd59ZnMLPR",
        "outputId": "2f7be71e-6af2-437e-ece8-517e6bbd057e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 7, 9, 11, 12, 14, 15, 19, 21, 22, 23, 25, 29, 30, 34, 35, 36, 37, 38, 45]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Columns to Extract\n",
        "The code initializes a list called `columns_to_extract` that contains the indices of the columns to be extracted from the CSV file. The specific columns to be extracted are specified as `[0, 2, 4, 5, 14]`.\n",
        "\n",
        "## Extracting Data from CSV\n",
        "The code initializes an empty list called output to store the extracted data.\n",
        "\n",
        "The code opens the CSV file specified by the variable `csv_file` in read mode using the open function. It creates a CSV reader object using `csv.reader` to read the file contents.\n",
        "\n",
        "The code skips the header row of the CSV file using the next function to move the reader to the next row.\n",
        "\n",
        "A nested function called `get_data` is defined to extract the data from the CSV file based on the provided `row_numbers` and `olumn_numbers`.\n",
        "\n",
        "Within the `get_data` function, the code iterates over each row in the CSV file using a for loop and the enumerate function. For each row, it checks if the index `i + 1` is in the `row_numbers` list. If it is, the code extracts the values from the specified `column_numbers` and appends them to the data list.\n",
        "\n",
        "The `get_data` function returns the extracted data as a list.\n",
        "\n",
        "The code calls the get_data function with the `Place_list (row numbers)` and `columns_to_extract` parameters, and assigns the returned data to the variable data.\n",
        "\n",
        "## Creating Output Dictionary\n",
        "The code iterates over each row in the data list using a for loop. For each row, it creates a dictionary containing the extracted data with keys `\"name\", \"address\", \"rating\", \"total_review\", and \"url_photo\"`. The values are assigned based on the corresponding elements in the row.\n",
        "\n",
        "Each dictionary is appended to the output list.\n",
        "\n",
        "## Returning the Output\n",
        "The code returns the output list, which contains a list of dictionaries representing the extracted data from the CSV file."
      ],
      "metadata": {
        "id": "WjCY4EikcB_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def caffe_result(Place_list):\n",
        "  columns_to_extract = [0, 2, 4, 5, 14]\n",
        "  output = []\n",
        "\n",
        "  with open(csv_file, 'r') as file:\n",
        "      reader = csv.reader(file)\n",
        "      next(reader)  # Skip the header row\n",
        "\n",
        "      def get_data(row_numbers, column_numbers):\n",
        "          data = []\n",
        "          for i, row in enumerate(reader):\n",
        "              if i + 1 in row_numbers:\n",
        "                  row_data = [row[col] for col in column_numbers]\n",
        "                  data.append(row_data)\n",
        "          return data\n",
        "\n",
        "      data = get_data(Place_list, columns_to_extract)\n",
        "      for row in data:\n",
        "          output.append({\n",
        "                \"name\": row[0],\n",
        "                \"address\": row[1],\n",
        "                \"rating\": float(row[2]),\n",
        "                \"total_review\": int(row[3]),\n",
        "                \"url_photo\": row[4]\n",
        "            })\n",
        "\n",
        "      return output\n",
        "\n",
        "# print(caffe_result(Place_list))\n",
        "\n",
        "\n",
        "results = caffe_result(Place_list)\n",
        "for result in results:\n",
        "    print(result)\n",
        "\n",
        "print(type(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxSHXDnYJUep",
        "outputId": "8e7dcc50-88dc-481f-8878-6cd69980f145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'name': 'Filosofi Kopi Jogja', 'address': 'Jl. Pandhawa No.001/17, Tegal Rejo, Sariharjo, Kec. Ngaglik, Kabupaten Sleman, Daerah Istimewa Yogyakarta 55581, Indonesia', 'rating': 4.5, 'total_review': 10643, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DtZv8Cgp2loEb8jQsfDS56fNN3N26tqc7y7Xk10jj6kXtIQ_wblkMeleGlljFV7jyGajr69EfbkU3GDlVo69D22-GMRnj84KNY=s1600-w400'}\n",
            "{'name': 'Cokelat Klasik Cafe', 'address': 'Jalan Joyo Agung, Merjosari, Lowokwaru, Tlogomas, Kec. Lowokwaru, Kota Malang, Jawa Timur 65144, Indonesia', 'rating': 4.4, 'total_review': 6741, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Dvw5URkLOzQ3skAAAqK9jbsVKzBlPvIIeaXG5pe0EbOjuj1KBANlozk-azxZEEKOA4tA-88XbICHMdARdbrDU4yltuOtluyUg=s1600-w400'}\n",
            "{'name': 'Bukit Delight', 'address': 'Jl. Joyo Agung No.1, Tlogomas, Kec. Lowokwaru, Kota Malang, Jawa Timur 65144, Indonesia', 'rating': 4.5, 'total_review': 5333, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Dt2mpfqwMdzCKBski9jEu0drc192U1XoO-fcI0m6mnSI9VBSHfwGxy6k0UD3fpC9MbLzds8_646hwoNwH08BaeCEjUN_IiNvbg=s1600-w400'}\n",
            "{'name': 'Warkop Brewok II', 'address': 'Jl. Trs.Candi Mendut No.37, Mojolangu, Kec. Lowokwaru, Kota Malang, Jawa Timur 65140, Indonesia', 'rating': 4.6, 'total_review': 3787, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Duxy0WkmKZrCkx1RVN6YJJkVUG7Jz-SR_oLHBpeo5CYaZe1rNrG8GUN1jAKpgyQsZ6pd_jTmid6QAIOi7ti_IW_eRH1VLefnjM=s1600-w400'}\n",
            "{'name': 'SODUS - Soto Kudus Kedai Taman', 'address': 'Jl. Taman Gayungsari Timur No.7, Menanggal, Kec. Gayungan, Surabaya, Jawa Timur 60235, Indonesia', 'rating': 4.4, 'total_review': 4809, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DvdQY0ErY7OUzLv5IY0SIZwKrPWx-DbpKsgtDJxJQHP4yU0KBG2UwCrj8mc0TEbIN0sPOFULvnQdwgS7aBotjOizkq5fOAgUjg=s1600-w400'}\n",
            "{'name': 'Baraka Coffee House', 'address': 'Jl. Watumujur II No.6, Ketawanggede, Kec. Lowokwaru, Kota Malang, Jawa Timur 65145, Indonesia', 'rating': 5.0, 'total_review': 1273, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DuWtPFa8LUJWn7ciCP-_qWy3v0Z_kxoBMezyYFIYwRWwPT5BRGwWPrahK3RZ6F6AdyfLht3UfKuIuxX4nWR4jBktPUMv-YGSe4=s1600-w400'}\n",
            "{'name': 'Starbucks Gubeng', 'address': 'Jl. Raya Gubeng No.33, RT.002/RW.06, Gubeng, Kec. Gubeng, Surabaya, Jawa Timur 60281, Indonesia', 'rating': 4.7, 'total_review': 2988, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Dv9k-l5mjy_Ur_ymrzT3Gg1UWQptgx9RVeg_CuLfkXOXY8zGGNKd1ssgMKGTKFXB9WsjdOn6vBR4-W1oVQKQhOXDOgBp73NqKU=s1600-w400'}\n",
            "{'name': 'Nderek Cafe', 'address': 'Mergelo No. 237, Mergelo, Gn. Gedangan, Kec. Magersari, Kota Mojokerto, Jawa Timur 61315, Indonesia', 'rating': 4.9, 'total_review': 1698, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DvWiayxxDboZnaIIdXkFm_rynhBBgug5rq0X2DqtKCU_A6rjASxsMHMpDYSmVrGL6PW4lNVc_PzXl51lO7m6EK_sIQ8WqGGX54=s1600-w400'}\n",
            "{'name': 'Amstirdam Coffee & Roastery', 'address': 'Ruko Soekarno Hatta Indah. D18, Mojolangu, Kec. Lowokwaru, Kota Malang, Jawa Timur 65142, Indonesia', 'rating': 4.7, 'total_review': 2774, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Duz-tLOgwnKCRydGpmU_btEIMzpTSd7Nq2J5NdhAGoiMi-tI_tqwelrXLFQMw2ezy5GaLF165emD4CiGKSkSpVoFylor2LgTrM=s1600-w400'}\n",
            "{'name': 'WARUNK WOW KWB', 'address': 'Jl. Panglima Sudirman No.39, Ngaglik, Kec. Batu, Kota Batu, Jawa Timur 65311, Indonesia', 'rating': 4.4, 'total_review': 4471, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DsSb1X-L--ZbxfEgNQu_Y6V8yNddLQf4l-isNQ_P2Kq6XjemFyBOO6IGLifeAktwZcfhLaTzoau4J7_DzncVqL9pLbEBlxg7n8=s1600-w400'}\n",
            "{'name': 'Retrorika - coffee bar & resto', 'address': 'Bumiaji, Batu City, East Java 65331, Indonesia', 'rating': 4.7, 'total_review': 2662, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DsOew-Loy9F756HDGNl6ccxbXOJCFB8v3J0tw8Tn3JPFiElvLnkIwQ-SGXOibItZIZNYoq0P5Mu8bnYgYGYHZfPvjxrDdjBGV0=s1600-w400'}\n",
            "{'name': 'My Kopi O!', 'address': 'Jl. Tenes No.14, Kauman, Kec. Klojen, Kota Malang, Jawa Timur 65119, Indonesia', 'rating': 4.6, 'total_review': 3146, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Dvf8rKXY0ZO0mI7u3GoAsogdncRar2YjYE0B9jfckIOyWxqZkk4MfvCU6vVPkwIcj8a9lqaRBojvWaK0S0nn6XFeaSAZHiG2Bo=s1600-w400'}\n",
            "{'name': 'Estuary Café', 'address': 'Gang Code 3, Karangjati Wetan, No.170 C, Monjali St, RT.06/RW.45, Gemawang, Sinduadi, Mlati, Sleman Regency, Special Region of Yogyakarta 55283, Indonesia', 'rating': 4.6, 'total_review': 3018, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DudfohlBUXdkfB4XvCaDbxwjmfI-9G6NX4PbVIYjIHB5QljRWVljQmgkRH44X9KCn0gKKZyKQta5Eg9DnIJoipOdBssXlvAlyc=s1600-w400'}\n",
            "{'name': 'Lafayette Coffee & Eatery', 'address': 'Jl. Semeru No.2, RW.4, Oro-oro Dowo, Kec. Klojen, Kota Malang, Jawa Timur 65119, Indonesia', 'rating': 4.6, 'total_review': 2742, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DtWs1oElH1mYHohGRNpCPGza0YWsaeekQq1jvfrvww6ZsvwUVDP4YaC0joorslKLTDKFLWWbtx9DmMAzWhxAX8m9AjW1APWiRs=s1600-w400'}\n",
            "{'name': 'Kase Coffee & Pool House', 'address': 'Jl. Kalimaya No.10, Tlogomas, Kec. Lowokwaru, Kota Malang, Jawa Timur 65144, Indonesia', 'rating': 5.0, 'total_review': 233, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DucYj4wFf44HGBkbEYsx1epRq7I92sFkG-kNvQ16E3LjpIrTlA6tmvIlFnVi-sKQ0ZcayGMce66t9Eqroz9hK7wzyGxl7M9_Pw=s1600-w400'}\n",
            "{'name': 'Noise Coffee', 'address': 'Jl. Sulfat, Pandanwangi, Kec. Blimbing, Kota Malang, Jawa Timur, Indonesia', 'rating': 5.0, 'total_review': 198, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Dv6isvFmNLn6OvVInOwQeUyTqwr2WyA1YYkvjn7RX0m0LjrS7ndEG4lYbQL_ov0TLh9zXA2kynQ6ax5GVdD4RaLILRNi_97eMs=s1600-w400'}\n",
            "{'name': 'Bendino Beverages', 'address': 'Fisip, Universitas Brawijaya, Ketawanggede, Kec. Lowokwaru, Kota Malang, Jawa Timur 65145, Indonesia', 'rating': 5.0, 'total_review': 180, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DvsUlxa7cyxeoUIx6nI2yFfxIm0JAsJP6nZ4fh6pxhuvEIs3Kjpmk2JyOu3xuG5bo0aZWEBu31acA9WKReq0fpStw2op3KeM_4=s1600-w400'}\n",
            "{'name': 'Starbucks Coffee Malang City Point', 'address': 'Lt. GF, Malang City Point, Jl. Terusan Dieng No.31, Pisang Candi, Kec. Sukun, Kota Malang, Jawa Timur 65146, Indonesia', 'rating': 4.6, 'total_review': 2460, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DtEuXnSMHaBi1w40S6xS1W1QXaYluA6ITv8BAXjA3TdWIfABdGtqiTrTLeR52qD9D5y0XBVu9yvaEZgtreiznIMPcXLDxfm0QY=s1600-w400'}\n",
            "{'name': 'Robucca', 'address': 'Blok.D, Ijen Nirwana, Jl. Raya No.1A, Bareng, Kec. Klojen, Kota Malang, Jawa Timur 65116, Indonesia', 'rating': 4.6, 'total_review': 2444, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3Dt6H5RmZIbF90w8Dk_IY4ae0oAKW6E8xlsc5fyQLz1ZRCtLsh0L0QfqLyxmukLyxVhZ44Qjw49kvwMDdFQLo4kvQijW0Wgukeg=s1600-w400'}\n",
            "{'name': 'GoMik Malang', 'address': 'Jl. Tapaksiring No.45, Samaan, Kec. Klojen, Kota Malang, Jawa Timur 65112, Indonesia', 'rating': 5.0, 'total_review': 62, 'url_photo': 'https://lh3.googleusercontent.com/places/ANJU3DsZtJZdnnn9lJTFP994WeWWExcJs1Al85bpLx9UGSvyE-37mnSwkPwHoIkXr-8aQTzxhSTPRPBsWnHB-hxALiT2sTzovDmdX9Q=s1600-w400'}\n",
            "<class 'dict'>\n"
          ]
        }
      ]
    }
  ]
}