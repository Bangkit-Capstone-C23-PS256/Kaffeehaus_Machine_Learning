{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rJHVbdeA3_hU"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import chardet\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw10ciJSMqdI",
        "outputId": "417abcca-adf3-487c-e293-0ed59978f189"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x44av-xVGBke",
        "outputId": "2bb0b9d1-f33e-4699-e8ea-b92c71df311b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load GloVe word embeddings\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/For Capstone/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n"
      ],
      "metadata": {
        "id": "DL8enUU24G6w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create embedding matrix and vocabulary\n",
        "embedding_dim = 100  # Dimensionality of the word embeddings\n",
        "embedding_matrix = np.zeros((len(embeddings_index), embedding_dim))\n",
        "vocab = []\n",
        "\n",
        "for i, word in enumerate(embeddings_index):\n",
        "    embedding_vector = embeddings_index[word]\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    vocab.append(word)"
      ],
      "metadata": {
        "id": "q0GOW9aIHCkz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Convert embedding matrix to TensorFlow embedding\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    len(embeddings_index),\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "PMmqjDKmHE7p"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Test the TensorFlow embedding\n",
        "word_input = tf.keras.Input(shape=(1,), dtype=tf.int32)\n",
        "embedding_output = embedding_layer(word_input)\n"
      ],
      "metadata": {
        "id": "Y6dhAxGeHHCj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build and compile a model that uses the embedding layer\n",
        "model = tf.keras.models.Sequential([\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "fBQCnBQ5HPEJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare the search data\n",
        "search_data = []\n",
        "for word in vocab:\n",
        "    search_data.append(embeddings_index[word])\n",
        "\n",
        "search_data = np.array(search_data)\n"
      ],
      "metadata": {
        "id": "wNt0kQ2KLKSF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the search function\n",
        "def search(query, top_k=5):\n",
        "    top_words_list = []\n",
        "    for query_word in query:\n",
        "        query_tokens = query_word.split()\n",
        "        query_embedding = np.mean([embeddings_index[token] for token in query_tokens if token in embeddings_index], axis=0)\n",
        "        similarity_scores = cosine_similarity([query_embedding], search_data)\n",
        "        similarity_scores = similarity_scores.reshape(-1)\n",
        "        top_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
        "        top_words = [vocab[i] for i in top_indices]\n",
        "        top_words_list.append(top_words)\n",
        "    return top_words_list\n"
      ],
      "metadata": {
        "id": "iYviKf_qLN2T"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Words to remove\n",
        "additional_keywords = [\"caffe\", \"place\", \"coffee\", \"nan\", \"cafe\"]\n",
        "\n",
        "# Asking User Input\n",
        "user_input = input(\"Search: \")\n",
        "\n",
        "# Split the input into individual words\n",
        "words = user_input.split()\n",
        "\n",
        "# Preprocess the search query by splitting and removing symbols\n",
        "search_keywords = re.findall(r'\\b\\w+\\b', user_input.lower())\n",
        "\n",
        "# Remove stop words and additional keywords from the search keywords\n",
        "search_keywords = [word for word in search_keywords if word not in stop_words and word not in additional_keywords]\n",
        "\n",
        "# Join the remaining words back together\n",
        "\n",
        "print(\"Filtered Input:\", search_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6uRo-8jMUBP",
        "outputId": "0e188193-8479-470a-bb6d-5882df93bbaf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search: cozy, affordable, cheap\n",
            "Filtered Input: ['cozy', 'affordable', 'cheap']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_list = search(search_keywords, top_k=10)\n",
        "\n",
        "list_of_words = []\n",
        "for i, query_word in enumerate(search_keywords):\n",
        "    print(f\"Closest meanings for query '{query_word}':\")\n",
        "    print(top_words_list[i])\n",
        "    list_of_words.append(top_words_list[i])\n",
        "    print()\n",
        "list_of_words  = [word for sublist in list_of_words for word in sublist]\n",
        "# list_of_words = list_of_words[0:len(query)]\n",
        "print('List closest keyword from query: \\n', list_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWnDxY5MM--k",
        "outputId": "b2d5268d-7bb3-4fd2-c492-0538f793b30f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest meanings for query 'cozy':\n",
            "['cozy', 'cosy', 'comfy', 'homey', 'cramped', 'dingy', 'spacious', 'clubby', 'rustic', 'shabby']\n",
            "\n",
            "Closest meanings for query 'affordable':\n",
            "['affordable', 'inexpensive', 'cheap', 'expensive', 'cheaper', 'efficient', 'unaffordable', 'accessible', 'priced', 'environmentally']\n",
            "\n",
            "Closest meanings for query 'cheap':\n",
            "['cheap', 'inexpensive', 'cheaper', 'expensive', 'affordable', 'easy', 'plentiful', 'pricey', 'buying', 'scarce']\n",
            "\n",
            "List closest keyword from query: \n",
            " ['cozy', 'cosy', 'comfy', 'homey', 'cramped', 'dingy', 'spacious', 'clubby', 'rustic', 'shabby', 'affordable', 'inexpensive', 'cheap', 'expensive', 'cheaper', 'efficient', 'unaffordable', 'accessible', 'priced', 'environmentally', 'cheap', 'inexpensive', 'cheaper', 'expensive', 'affordable', 'easy', 'plentiful', 'pricey', 'buying', 'scarce']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_keywords(csv_file, keywords, column):\n",
        "\n",
        "  with open(csv_file, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = []\n",
        "    for row in reader:\n",
        "      rows.append(row)\n",
        "\n",
        "  row_numbers = []\n",
        "  for row in rows:\n",
        "    for keyword in keywords:\n",
        "      if keyword in row[column]:\n",
        "        row_numbers.append(rows.index(row))\n",
        "\n",
        "  return row_numbers\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  csv_file = '/content/drive/MyDrive/For Capstone/Place Detail - Hasil_Ekstraksi(One Keyword) + Reranked.csv'\n",
        "  keywords = list_of_words\n",
        "  column = 13\n",
        "\n",
        "  row_numbers = search_keywords(csv_file, keywords, column)\n",
        "  unique_list = list(set(row_numbers))\n",
        "  sorted_list = sorted(unique_list)\n",
        "  Place_list= sorted_list[:20]\n",
        "  print(Place_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPcd59ZnMLPR",
        "outputId": "58825f0c-df82-45df-cd4d-41ffe2984a06"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 7, 10, 11, 14, 15, 23, 24, 25, 26, 28, 29, 32, 33, 34, 35, 36, 37, 41, 47]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_extract = [0, 2, 4]\n",
        "\n",
        "with open(csv_file, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # Skip the header row\n",
        "\n",
        "    def get_data(row_numbers, column_numbers):\n",
        "        data = []\n",
        "        for i, row in enumerate(reader):\n",
        "            if i + 1 in row_numbers:\n",
        "                row_data = [row[col] for col in column_numbers]\n",
        "                data.append(row_data)\n",
        "        return data\n",
        "\n",
        "    data = get_data(Place_list, columns_to_extract)\n",
        "    for row in data:\n",
        "        print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxSHXDnYJUep",
        "outputId": "8b50cd5a-c302-4bf8-c58d-f3dbbf4f187f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cokelat Klasik Cafe', 'Jalan Joyo Agung, Merjosari, Lowokwaru, Tlogomas, Kec. Lowokwaru, Kota Malang, Jawa Timur 65144, Indonesia', '4.4']\n",
            "['Baraka Coffee House', 'Jl. Watumujur II No.6, Ketawanggede, Kec. Lowokwaru, Kota Malang, Jawa Timur 65145, Indonesia', '5.0']\n",
            "['My Kopi O!', 'Jl. Tenes No.14, Kauman, Kec. Klojen, Kota Malang, Jawa Timur 65119, Indonesia', '4.6']\n",
            "['Lafayette Coffee & Eatery', 'Jl. Semeru No.2, RW.4, Oro-oro Dowo, Kec. Klojen, Kota Malang, Jawa Timur 65119, Indonesia', '4.6']\n",
            "['Noise Coffee', 'Jl. Sulfat, Pandanwangi, Kec. Blimbing, Kota Malang, Jawa Timur, Indonesia', '5.0']\n",
            "['Bendino Beverages', 'Fisip, Universitas Brawijaya, Ketawanggede, Kec. Lowokwaru, Kota Malang, Jawa Timur 65145, Indonesia', '5.0']\n",
            "['Kuma Bake and Coffee', 'Kantin Fakultas Teknologi Pertanian Universitas Brawijaya, Jl. Veteran, Ketawanggede, Kec. Lowokwaru, Kota Malang, Jawa Timur 65145, Indonesia', '5.0']\n",
            "['FF GARDENSPACE', 'Jl. Karanglo Indah Atas No.12, Jajar, Tanjungtirto, Kec. Singosari, Kabupaten Malang, Jawa Timur 65126, Indonesia', '5.0']\n",
            "['Warkop Bento', 'Jl. Karanglo Indah, Balearjosari, Kec. Blimbing, Kota Malang, Jawa Timur 65126, Indonesia', '5.0']\n",
            "['Kopi Very', 'Jl. Ikan Kakap No.2A, Tunjungsekar, Kec. Lowokwaru, Kota Malang, Jawa Timur 65142, Indonesia', '5.0']\n",
            "['Kedai Jamyth', 'Tlk. Mandar I No.36E, Arjosari, Kec. Blimbing, Kota Malang, Jawa Timur 65126, Indonesia', '5.0']\n",
            "['Kedai Tjakalang', 'Jl. Cakalang, Polowijen, Kec. Blimbing, Kota Malang, Jawa Timur 65125, Indonesia', '5.0']\n",
            "['Akong Kedai (Noodle & Coffee)', 'Jl. Gladiol No.9a, Jatimulyo, Kec. Lowokwaru, Kota Malang, Jawa Timur 65141, Indonesia', '5.0']\n",
            "['Calathea Garden', 'Jl. Arif Rahman Hakim No.IV No. 129, Kauman, Kec. Klojen, Kota Malang, Jawa Timur 65119, Indonesia', '5.0']\n",
            "['Sabatikal Barbershop', 'Jl. MT. Haryono No.63, Dinoyo, Kec. Lowokwaru, Kota Malang, Jawa Timur 65144, Indonesia', '5.0']\n",
            "['Sins Coffee', 'Kav. 2, Jl. Sudimoro Utara, Mojolangu, Kec. Lowokwaru, Kota Malang, Jawa Timur 65142, Indonesia', '5.0']\n",
            "['Burger Bingo!', 'Jl. Laksda Adi Sucipto No.367, Pandanwangi, Kec. Blimbing, Kota Malang, Jawa Timur 65126, Indonesia', '5.0']\n",
            "['Kedai Wayank', 'Jl. Keben II Blk. C No.9, Bandungrejosari, Kec. Sukun, Kota Malang, Jawa Timur 65148, Indonesia', '5.0']\n",
            "['Pangkubudhe', 'Jl. Raya Dermo No.168, Dermo, Mulyoagung, Kec. Dau, Kabupaten Malang, Jawa Timur 65151, Indonesia', '5.0']\n",
            "['Pokok Ngopi', '3J8H+4FJ, Mojolangu, Lowokwaru, Malang City, East Java 65142, Indonesia', '5.0']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('Word_Embedding.h5')"
      ],
      "metadata": {
        "id": "L0V5nC7uMdSN"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained TensorFlow model\n",
        "model = tf.keras.models.load_model('Word_Embedding.h5')  # Replace 'your_model.h5' with your model file\n",
        "\n",
        "# Convert the model to JSON\n",
        "model_json = model.to_json()\n",
        "\n",
        "# Save the JSON model to a file\n",
        "with open('Word_Embedding.json', 'w') as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "metadata": {
        "id": "Lm_K5qEgMdx9"
      },
      "execution_count": 53,
      "outputs": []
    }
  ]
}