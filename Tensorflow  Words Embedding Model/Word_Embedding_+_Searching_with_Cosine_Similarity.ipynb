{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rJHVbdeA3_hU"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import chardet\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw10ciJSMqdI",
        "outputId": "e710ca22-1028-46cc-a0ff-74ae65cb82a1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x44av-xVGBke",
        "outputId": "c5730086-d917-45d3-d625-49b278f54842"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load GloVe word embeddings\n",
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/glove.6B.100d.txt', 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n"
      ],
      "metadata": {
        "id": "DL8enUU24G6w"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create embedding matrix and vocabulary\n",
        "embedding_dim = 100  # Dimensionality of the word embeddings\n",
        "embedding_matrix = np.zeros((len(embeddings_index), embedding_dim))\n",
        "vocab = []\n",
        "\n",
        "for i, word in enumerate(embeddings_index):\n",
        "    embedding_vector = embeddings_index[word]\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    vocab.append(word)"
      ],
      "metadata": {
        "id": "q0GOW9aIHCkz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Convert embedding matrix to TensorFlow embedding\n",
        "embedding_layer = tf.keras.layers.Embedding(\n",
        "    len(embeddings_index),\n",
        "    embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    trainable=False\n",
        ")"
      ],
      "metadata": {
        "id": "PMmqjDKmHE7p"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Test the TensorFlow embedding\n",
        "word_input = tf.keras.Input(shape=(1,), dtype=tf.int32)\n",
        "embedding_output = embedding_layer(word_input)\n"
      ],
      "metadata": {
        "id": "Y6dhAxGeHHCj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Build and compile a model that uses the embedding layer\n",
        "model = tf.keras.models.Sequential([\n",
        "    embedding_layer,\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "fBQCnBQ5HPEJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "\n",
        "word = \"machine\"\n",
        "word_index = vocab.index(word)\n",
        "embedded_word = embedding_matrix[word_index]\n",
        "print(embedded_word)"
      ],
      "metadata": {
        "id": "JoFTjJamwG5j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5565829e-39c2-4c12-d5c7-17ef91543015"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.65364999  0.49419001 -0.26245001 -0.20722    -0.11413     0.35701001\n",
            "  1.04540002  0.21881001  0.52768999  0.60606003  0.42521    -0.65169001\n",
            "  0.15318    -0.14797001  0.12650999 -0.017124    0.45324999  0.37165999\n",
            " -0.26846999 -0.26269999  0.43869001 -0.016615    0.12714    -0.54707998\n",
            "  0.089084    0.24336    -0.34415001  0.0026505  -0.094268    0.056114\n",
            "  0.46366     0.68786001 -0.20631    -0.088003    0.32153001 -0.91399002\n",
            " -0.080976   -0.90761     0.92888999 -0.68032998  0.23801    -0.37469\n",
            " -0.43278    -0.19243    -0.23711    -0.73040998 -0.50591999 -0.30237001\n",
            "  0.0017281  -0.60922998 -0.21046001  0.47402999  0.37333     1.24749994\n",
            "  0.62989998 -1.52919996 -0.32403001  0.59680998  0.97994     0.59755999\n",
            "  0.67624998  0.28222999 -0.26747999  1.42499995 -0.34419     0.25211999\n",
            "  0.30239999 -0.26582    -0.22583     0.53783    -0.44439    -0.24281\n",
            "  0.38001001  0.085317    0.49693999  0.24058001  0.20611     0.023896\n",
            " -0.53078002  0.12086     1.16270006 -0.0053908  -0.66131997  0.073666\n",
            " -1.59870005  0.3626      0.68496001 -0.93403     0.30522999 -0.1688\n",
            "  0.43895     0.73641002  0.56431001  1.08039999  0.074377   -0.89155\n",
            " -0.20935    -0.30410001  1.30270004  0.12729999]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Prepare the search data\n",
        "search_data = []\n",
        "for word in vocab:\n",
        "    search_data.append(embeddings_index[word])\n",
        "\n",
        "search_data = np.array(search_data)\n"
      ],
      "metadata": {
        "id": "wNt0kQ2KLKSF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Define the search function\n",
        "def search(query, top_k=5):\n",
        "    top_words_list = []\n",
        "    for query_word in query:\n",
        "        query_tokens = query_word.split()\n",
        "        query_embedding = np.mean([embeddings_index[token] for token in query_tokens if token in embeddings_index], axis=0)\n",
        "        similarity_scores = cosine_similarity([query_embedding], search_data)\n",
        "        similarity_scores = similarity_scores.reshape(-1)\n",
        "        top_indices = similarity_scores.argsort()[-top_k:][::-1]\n",
        "        top_words = [vocab[i] for i in top_indices]\n",
        "        top_words_list.append(top_words)\n",
        "    return top_words_list\n"
      ],
      "metadata": {
        "id": "iYviKf_qLN2T"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Words to remove\n",
        "additional_keywords = [\"caffe\", \"place\", \"coffee\", \"nan\", \"cafe\"]\n",
        "\n",
        "# Asking User Input\n",
        "user_input = input(\"Search: \")\n",
        "\n",
        "# Split the input into individual words\n",
        "words = user_input.split()\n",
        "\n",
        "# Preprocess the search query by splitting and removing symbols\n",
        "search_keywords = re.findall(r'\\b\\w+\\b', user_input.lower())\n",
        "\n",
        "# Remove stop words and additional keywords from the search keywords\n",
        "search_keywords = [word for word in search_keywords if word not in stop_words and word not in additional_keywords]\n",
        "\n",
        "# Join the remaining words back together\n",
        "\n",
        "print(\"Filtered Input:\", search_keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6uRo-8jMUBP",
        "outputId": "0be67e68-2acc-4434-c7a4-f4d62bbe6dcc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search: caffe that has a wifi and beautiful view\n",
            "Filtered Input: ['wifi', 'beautiful', 'view']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_list = search(search_keywords, top_k=10)\n",
        "\n",
        "list_of_words = []\n",
        "for i, query_word in enumerate(search_keywords):\n",
        "    print(f\"Closest meanings for query '{query_word}':\")\n",
        "    print(top_words_list[i])\n",
        "    list_of_words.append(top_words_list[i])\n",
        "    print()\n",
        "list_of_words  = [word for sublist in list_of_words for word in sublist]\n",
        "# list_of_words = list_of_words[0:len(query)]\n",
        "print('List closest keyword from query: \\n', list_of_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWnDxY5MM--k",
        "outputId": "b45b6508-f9bb-4da4-b43c-041bbe2ffc48"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Closest meanings for query 'wifi':\n",
            "['wifi', 'wi-fi', 'bluetooth', 'connectivity', '802.11', 'wireless', 'wi', 'broadband', 'wimax', 'wlan']\n",
            "\n",
            "Closest meanings for query 'beautiful':\n",
            "['beautiful', 'lovely', 'gorgeous', 'wonderful', 'charming', 'magnificent', 'elegant', 'fabulous', 'splendid', 'perfect']\n",
            "\n",
            "Closest meanings for query 'view':\n",
            "['view', 'views', 'fact', 'approach', 'viewed', 'clear', 'see', 'notion', 'indeed', 'what']\n",
            "\n",
            "List closest keyword from query: \n",
            " ['wifi', 'wi-fi', 'bluetooth', 'connectivity', '802.11', 'wireless', 'wi', 'broadband', 'wimax', 'wlan', 'beautiful', 'lovely', 'gorgeous', 'wonderful', 'charming', 'magnificent', 'elegant', 'fabulous', 'splendid', 'perfect', 'view', 'views', 'fact', 'approach', 'viewed', 'clear', 'see', 'notion', 'indeed', 'what']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def search_keywords(csv_file, keywords, column):\n",
        "\n",
        "  with open(csv_file, 'r') as f:\n",
        "    reader = csv.reader(f)\n",
        "    rows = []\n",
        "    for row in reader:\n",
        "      rows.append(row)\n",
        "\n",
        "  row_numbers = []\n",
        "  for row in rows:\n",
        "    for keyword in keywords:\n",
        "      if keyword in row[column]:\n",
        "        row_numbers.append(rows.index(row))\n",
        "\n",
        "  return row_numbers\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  csv_file = '/content/drive/MyDrive/For Capstone/Collecting data/Place Detail (Scored + Keyword 1 & 2 Extracted  + Additional Feature (longlang, contact etc)).csv'\n",
        "  keywords = list_of_words\n",
        "  column = 13\n",
        "\n",
        "  row_numbers = search_keywords(csv_file, keywords, column)\n",
        "  unique_list = list(set(row_numbers))\n",
        "  sorted_list = sorted(unique_list)\n",
        "  Place_list= sorted_list[:20]\n",
        "  print(Place_list)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPcd59ZnMLPR",
        "outputId": "d43907c8-fb46-47a3-8e3f-d6ba8c19733a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6, 10, 14, 17, 25, 37, 39, 53, 55, 57, 70, 71, 75, 76, 85, 87, 88, 93, 94, 96]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_extract = [0, 2, 4, 5]\n",
        "\n",
        "with open(csv_file, 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader)  # Skip the header row\n",
        "\n",
        "    def get_data(row_numbers, column_numbers):\n",
        "        data = []\n",
        "        for i, row in enumerate(reader):\n",
        "            if i + 1 in row_numbers:\n",
        "                row_data = [row[col] for col in column_numbers]\n",
        "                data.append(row_data)\n",
        "        return data\n",
        "\n",
        "    data = get_data(Place_list, columns_to_extract)\n",
        "    for row in data:\n",
        "        print(row)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxSHXDnYJUep",
        "outputId": "98288032-7c8f-44d7-b5c3-8cbb82dac5bc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Loko Coffee Shop Malioboro Yogyakarta', 'Komp. Ps. Kembang, Jl. Ps. Kembang, Sosromenduran, Gedong Tengen, Kota Yogyakarta, Daerah Istimewa Yogyakarta 55271, Indonesia', '4.5', '6488']\n",
            "['Pupuk Bawang Batu', 'Jl. Panglima Sudirman No.116, Pesanggrahan, Kec. Batu, Kota Batu, Jawa Timur 55313, Indonesia', '4.2', '6542']\n",
            "['Baraka Coffee House', 'Jl. Watumujur II No.6, Ketawanggede, Kec. Lowokwaru, Kota Malang, Jawa Timur 65145, Indonesia', '5', '1273']\n",
            "['Java Dancer Coffee - Jakarta', 'Jl. Jakarta No.59, Oro-oro Dowo, Kec. Klojen, Kota Malang, Jawa Timur 65112, Indonesia', '4.5', '4085']\n",
            "['My Kopi O!', 'Jl. Tenes No.14, Kauman, Kec. Klojen, Kota Malang, Jawa Timur 65119, Indonesia', '4.6', '3146']\n",
            "['Starbucks Coffee Malang City Point', 'Lt. GF, Malang City Point, Jl. Terusan Dieng No.31, Pisang Candi, Kec. Sukun, Kota Malang, Jawa Timur 65146, Indonesia', '4.6', '2460']\n",
            "['Cokelat Klasik Cafe Batu', '4GP8+H8P, Pesanggrahan, Batu, Batu City, East Java 65313, Indonesia', '4.4', '3580']\n",
            "['Kedai Jamyth', 'Tlk. Mandar I No.36E, Arjosari, Kec. Blimbing, Kota Malang, Jawa Timur 65126, Indonesia', '5', '50']\n",
            "['Kedai Tjakalang', 'Jl. Cakalang, Polowijen, Kec. Blimbing, Kota Malang, Jawa Timur 65125, Indonesia', '5', '44']\n",
            "['Omah Lawas', 'Jl. Mayjend Panjaitan No.17, Penanggungan, Kec. Klojen, Kota Malang, Jawa Timur 65113, Indonesia', '5', '43']\n",
            "['Kamsia boba & chiken Gajahmada', '2J8J+8VQ, Jl. Agus Salim, Sukoharjo, Kec. Klojen, Kota Malang, Jawa Timur 65118, Indonesia', '5', '35']\n",
            "['OBOE coffee', 'Jl. I.R. Rais No.220, Bareng, Kec. Klojen, Kota Malang, Jawa Timur 65116, Indonesia', '5', '35']\n",
            "['Toko kopi sudawirat', 'Jl. Kedawung No.35, Tulusrejo, Kec. Lowokwaru, Kota Malang, Jawa Timur 65141, Indonesia', '5', '31']\n",
            "['Penthouse Dorm&Cafe', 'Perumahan Jl. Taman Indah Soekarno Hatta No.16, Mojolangu, Kec. Lowokwaru, Kota Malang, Jawa Timur 65142, Indonesia', '5', '31']\n",
            "['Ngopi time nukus', 'Jl. Sukun Gempol No.4, Sukun, Kec. Sukun, Kota Malang, Jawa Timur 65147, Indonesia', '5', '26']\n",
            "['Karta Coffee & Eatery', 'Unit 13-16, Plaza Malioboro L1, Jl. Malioboro No.52-58, Suryatmajan, Kec. Danurejan, Kota Yogyakarta, Daerah Istimewa Yogyakarta 55213, Indonesia', '5', '25']\n",
            "['Koffie Receipt', 'Jl. Ikan Tombro, Mojolangu, Kec. Lowokwaru, Kota Malang, Jawa Timur 65142, Indonesia', '5', '25']\n",
            "['Cepit Kopi', 'Jl. Bandulan No.143, Bandulan, Kec. Sukun, Kota Malang, Jawa Timur 65146, Indonesia', '5', '23']\n",
            "['OMAH PUNDEN', 'Jl. Raya Tumapel Gg. 4 No.22, Pangetan, Pagentan, Kec. Singosari, Kabupaten Malang, Jawa Timur 65126, Indonesia', '5', '23']\n",
            "['Kedai Kreewools', 'Tengger Kadangan XVIII No.8, Kandangan, Kec. Benowo, Surabaya, Jawa Timur 60199, Indonesia', '5', '22']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/Word_Embedding.h5')"
      ],
      "metadata": {
        "id": "L0V5nC7uMdSN"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your trained TensorFlow model\n",
        "model = tf.keras.models.load_model('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/Word_Embedding.h5')  # Replace 'your_model.h5' with your model file\n",
        "\n",
        "# Convert the model to JSON\n",
        "model_json = model.to_json()\n",
        "\n",
        "# Save the JSON model to a file\n",
        "with open('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/Word_Embedding.json', 'w') as json_file:\n",
        "    json_file.write(model_json)"
      ],
      "metadata": {
        "id": "Lm_K5qEgMdx9"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cp -r \"/content/Word_Embedding.h5\" \"/content/drive/MyDrive/For Capstone\"\n",
        "#!cp -r \"/content/Word_Embedding.json\" \"/content/drive/MyDrive/For Capstone\"\n"
      ],
      "metadata": {
        "id": "Ul8_YzQ0NLC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = '/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding'\n",
        "\n",
        "# YOUR CODE HERE\n",
        "tf.saved_model.save(model, export_dir = export_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85jLGnJNQ564",
        "outputId": "9db71c90-6263-460b-ddae-819538276430"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select mode of optimization\n",
        "mode = \"Speed\" \n",
        "\n",
        "if mode == 'Storage':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_SIZE\n",
        "elif mode == 'Speed':\n",
        "    optimization = tf.lite.Optimize.OPTIMIZE_FOR_LATENCY\n",
        "else:\n",
        "    optimization = tf.lite.Optimize.DEFAULT"
      ],
      "metadata": {
        "id": "-fQGSh0qQ9x4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# Set the optimzations\n",
        "converter.optimizations = [optimization]\n",
        "\n",
        "# Invoke the converter to finally generate the TFLite model\n",
        "tflite_model = converter.convert()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WClbLF-6RJDt",
        "outputId": "816e7948-b9ba-4092-cc6d-a25388fb1639"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Optimization option OPTIMIZE_FOR_LATENCY is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_LATENCY is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n",
            "WARNING:absl:Optimization option OPTIMIZE_FOR_LATENCY is deprecated, please use optimizations=[Optimize.DEFAULT] instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_model_file = pathlib.Path('/content/drive/MyDrive/For Capstone/Tensorflow Words Embedding/model for Word Embedding + Searching with Cosine Similarity.tflite')\n",
        "tflite_model_file.write_bytes(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pBi4KNyRLya",
        "outputId": "a7b79ae9-de07-4da7-deee-7413fefaa3ad"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40015920"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nyKKh4nQielS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!cp -r \"/content/model for Word Embedding + Searching with Cosine Similarity.tflite\" \"/content/drive/MyDrive/For Capstone\""
      ],
      "metadata": {
        "id": "szSrqapOTYr2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}